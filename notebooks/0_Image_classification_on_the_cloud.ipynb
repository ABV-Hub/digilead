{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6ZDpd9XzFeN"
   },
   "source": [
    "# Image classification with TensorFlow \n",
    "\n",
    "Based on an original notebook by the TensorFlow authors, licensed under Apache 2.0.\n",
    "\n",
    "**Note: You need to sign in to Google to run this notebook.**\n",
    "\n",
    "Use **Shift + Enter** to run the cells. When prompted, click **Run anyway** then **Yes**. Try it on this cell..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "PYndYo05fjzd",
    "outputId": "e8267105-64ca-4307-e338-4fa49c1fc493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check runtime type!\n"
     ]
    }
   ],
   "source": [
    "print(\"Check runtime type!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNo1Vfghpa8j"
   },
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, we're going to classify some images of fossils.\n",
    "\n",
    "A 'notebook' is an interactive coding and note-taking environment. We're going to be using some cutting edge technology, right in your browser. We will see:\n",
    "\n",
    "- A deep neural network in action.\n",
    "- Google's TensorFlow deep learning library.\n",
    "- Google's 'tensor processing unit' (TPU) deep learning hardware acceleration.\n",
    "- All of this is running on Google's infrastructure, for free.\n",
    "\n",
    "There are fewer than 100 lines of code altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lvo0t7XVIkWZ"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MICrRv8rmXVq"
   },
   "source": [
    "We'll begin by downloading the dataset. Run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "i9aTiz206S68",
    "outputId": "21b1b6f6-19d6-4d27-b5e6-ad578558af04"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "\n",
    "X_ = requests.get(\"https://s3.amazonaws.com/agilegeo/geocomp/image_X.npy\")\n",
    "y_ = requests.get(\"https://s3.amazonaws.com/agilegeo/geocomp/integer_y.npy\")\n",
    "\n",
    "X = np.load(BytesIO(X_.content))\n",
    "y = np.load(BytesIO(y_.content))\n",
    "\n",
    "print(\"Data loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "Rupu_61y6tt7",
    "outputId": "33a52fab-4382-4d03-cb1f-39a4c2120432"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.316)\n",
    "\n",
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Ft7DuD_5--x-",
    "outputId": "7e05a9ee-8333-4654-d22a-feadce3966c6"
   },
   "outputs": [],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hgc2FZKVMx15"
   },
   "source": [
    "## Define the model\n",
    "\n",
    "The following example uses a standard conv-net that has 3 layers with drop-out and batch normalization between each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W7gMbs70GxA7"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_model(input_shape):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.BatchNormalization(input_shape=input_shape))\n",
    "    model.add(tf.keras.layers.Conv2D(64, (5, 5), padding='same', activation='elu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "    model.add(tf.keras.layers.BatchNormalization(input_shape=input_shape))\n",
    "    model.add(tf.keras.layers.Conv2D(32, (5, 5), padding='same', activation='elu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(32))\n",
    "    model.add(tf.keras.layers.Activation('elu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "    model.add(tf.keras.layers.Dense(3))\n",
    "    model.add(tf.keras.layers.Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xLeZATVaNAnE"
   },
   "source": [
    "## Train on the TPU\n",
    "\n",
    "To begin training, construct the model on the TPU and then compile it.\n",
    "\n",
    "The following code demonstrates the use of a generator function and `fit_generator` to train the model.  Alternately, you can pass in `x_train` and `y_train` to `tpu_model.fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "colab_type": "code",
    "id": "pWEYmd_hIWg8",
    "outputId": "49c66e55-e962-467c-9b3d-54b6d8665130"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\n",
    "try:\n",
    "    resolver = tf.contrib.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "    tf.contrib.distribute.initialize_tpu_system(resolver)\n",
    "    strategy = tf.contrib.distribute.TPUStrategy(resolver)\n",
    "except:\n",
    "    strategy = None\n",
    "\n",
    "none_context = contextmanager(lambda: iter([None]))()\n",
    "\n",
    "with (strategy.scope() if strategy else none_context):\n",
    "    model = create_model(input_shape=X_train.shape[1:])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train.astype(np.float32),\n",
    "    y_train.astype(np.float32),\n",
    "    epochs=32,\n",
    "    steps_per_epoch=50,\n",
    "    validation_data=(X_val.astype(np.float32), y_val.astype(np.float32)),\n",
    "    validation_freq=32\n",
    ")\n",
    "\n",
    "model.save_weights('./fossils.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ESL6ltQTMm05"
   },
   "source": [
    "## Check the results (inference)\n",
    "\n",
    "Now that you are done training, see how well the model can predict fossil types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SaYPv_aKId2d"
   },
   "outputs": [],
   "source": [
    "LABEL_NAMES = ['ammonites', 'fish', 'trilobites']\n",
    "\n",
    "cpu_model = create_model(X_train.shape[1:])\n",
    "cpu_model.load_weights('./fossils.h5')\n",
    "\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_predictions(images, predictions, actuals):\n",
    "    n = images.shape[0]\n",
    "    nc = int(np.ceil(n / 4))\n",
    "    f, axes = pyplot.subplots(nc, 4)\n",
    "    for i in range(nc * 4):\n",
    "        y = i // 4\n",
    "        x = i % 4\n",
    "        axes[x, y].axis('off')\n",
    "\n",
    "        pred = LABEL_NAMES[np.argmax(predictions[i])]\n",
    "        actual = LABEL_NAMES[actuals[i]]\n",
    "        conf = np.max(predictions[i])\n",
    "        if i > n:\n",
    "            continue\n",
    "        axes[x, y].imshow(images[i])\n",
    "        axes[x, y].set_title(\"{} {:.3f}\\n {}\".format(pred, conf, actual))\n",
    "\n",
    "    pyplot.gcf().set_size_inches(10, 12)  \n",
    "\n",
    "plot_predictions(np.squeeze(X_val[:16]), \n",
    "                 cpu_model.predict(X_val[:16]),\n",
    "                 np.squeeze(y_val[:16])\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LJSlWUxQiR-f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "0_Image_classification_on_the_cloud",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "deepl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
