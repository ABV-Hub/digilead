{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6ZDpd9XzFeN"
   },
   "source": [
    "# Image classification with TensorFlow \n",
    "\n",
    "Based on an original notebook by the TensorFlow authors, licensed under Apache 2.0.\n",
    "\n",
    "**Note: You need to sign in to Google to run this notebook.**\n",
    "\n",
    "Use **Shift + Enter** to run the cells. When prompted, click **Run anyway** then **Yes**. Try it on this cell..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PYndYo05fjzd",
    "outputId": "f5a775a7-f1e8-433e-f7d4-9700a150d90b"
   },
   "outputs": [],
   "source": [
    "print(\"It worked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNo1Vfghpa8j"
   },
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, we're going to classify some images of fossils.\n",
    "\n",
    "A 'notebook' is an interactive coding and note-taking environment. We're going to be using some cutting edge technology, right in your browser. We will see:\n",
    "\n",
    "- A deep neural network in action.\n",
    "- Google's TensorFlow deep learning library.\n",
    "- Google's 'tensor processing unit' (TPU) deep learning hardware acceleration.\n",
    "- All of this is running on Google's infrastructure, for free.\n",
    "\n",
    "There are fewer than 100 lines of code altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lvo0t7XVIkWZ"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MICrRv8rmXVq"
   },
   "source": [
    "We'll begin by downloading the dataset. Run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "i9aTiz206S68",
    "outputId": "30548b10-656d-4454-f598-6c4db1ddb453"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "\n",
    "X_ = requests.get(\"https://s3.amazonaws.com/agilegeo/geocomp/image_X.npy\")\n",
    "y_ = requests.get(\"https://s3.amazonaws.com/agilegeo/geocomp/integer_y.npy\")\n",
    "\n",
    "X = np.load(BytesIO(X_.content))\n",
    "y = np.load(BytesIO(y_.content))\n",
    "\n",
    "print(\"Data loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Rupu_61y6tt7",
    "outputId": "0bda8ed7-1a07-40d8-a816-5b8c206a579d"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hgc2FZKVMx15"
   },
   "source": [
    "## Define the model\n",
    "\n",
    "The following example uses a standard conv-net that has 3 layers with drop-out and batch normalization between each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 870
    },
    "colab_type": "code",
    "id": "W7gMbs70GxA7",
    "outputId": "85ad01a0-fa03-4869-850c-2203faede85b"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.BatchNormalization(input_shape=X_train.shape[1:]))\n",
    "model.add(tf.keras.layers.Conv2D(64, (5, 5), padding='same', activation='elu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.BatchNormalization(input_shape=X_train.shape[1:]))\n",
    "model.add(tf.keras.layers.Conv2D(128, (5, 5), padding='same', activation='elu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "# model.add(tf.keras.layers.BatchNormalization(input_shape=X_train.shape[1:]))\n",
    "# model.add(tf.keras.layers.Conv2D(256, (5, 5), padding='same', activation='elu'))\n",
    "# model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "# model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "# model.add(tf.keras.layers.Dense(256))\n",
    "model.add(tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.Activation('elu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(10))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xLeZATVaNAnE"
   },
   "source": [
    "## Train on the TPU\n",
    "\n",
    "To begin training, construct the model on the TPU and then compile it.\n",
    "\n",
    "The following code demonstrates the use of a generator function and `fit_generator` to train the model.  Alternately, you can pass in `x_train` and `y_train` to `tpu_model.fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "colab_type": "code",
    "id": "pWEYmd_hIWg8",
    "outputId": "31a570c8-872c-4ff6-abb9-e1cf62c92501"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
    "        model,\n",
    "        strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
    "            tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "        )\n",
    "    )\n",
    "except KeyError:  # os.environ fails if no TPU\n",
    "    tpu_model = model\n",
    "\n",
    "tpu_model.compile(\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=1e-3, ),\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    "\n",
    "def train_gen():\n",
    "    \"\"\"Training, no batches.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        yield X_train, y_train\n",
    "    \n",
    "\n",
    "tpu_model.fit_generator(\n",
    "    train_gen(),\n",
    "    epochs=1,\n",
    "    steps_per_epoch=1000,\n",
    "    validation_data=(X_val, y_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ESL6ltQTMm05"
   },
   "source": [
    "## Check the results (inference)\n",
    "\n",
    "Now that you are done training, see how well the model can predict fossil types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "colab_type": "code",
    "id": "SaYPv_aKId2d",
    "outputId": "a7479907-c7d0-4786-81f4-18286f93adf4"
   },
   "outputs": [],
   "source": [
    "LABEL_NAMES = ['ammonites', 'fish', 'trilobites']\n",
    "\n",
    "try:\n",
    "    cpu_model = tpu_model.sync_to_cpu()\n",
    "except AttributeError:\n",
    "    cpu_model = tpu_model  # We were never on TPUs\n",
    "\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_predictions(images, predictions, actuals):\n",
    "    n = images.shape[0]\n",
    "    nc = int(np.ceil(n / 4))\n",
    "    f, axes = pyplot.subplots(nc, 4)\n",
    "    for i in range(nc * 4):\n",
    "        y = i // 4\n",
    "        x = i % 4\n",
    "        axes[x, y].axis('off')\n",
    "\n",
    "        pred = LABEL_NAMES[np.argmax(predictions[i])]\n",
    "        actual = LABEL_NAMES[actuals[i]]\n",
    "        conf = np.max(predictions[i])\n",
    "        if i > n:\n",
    "            continue\n",
    "        axes[x, y].imshow(images[i])\n",
    "        axes[x, y].set_title(\"{} {:.3f}\\n {}\".format(pred, conf, actual))\n",
    "\n",
    "    pyplot.gcf().set_size_inches(10, 12)  \n",
    "\n",
    "plot_predictions(np.squeeze(X_val[:16]), \n",
    "                 tpu_model.predict(X_val[:16]),\n",
    "                 np.squeeze(y_val[:16])\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LJSlWUxQiR-f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "0_Image_classification_on_the_cloud",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "deepl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
